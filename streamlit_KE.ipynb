{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIK2Al2jNEpmaWgDUD/xp+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/InTheBOX322/gdp-dashboard/blob/main/streamlit_KE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUTHDjeInG5q",
        "outputId": "98a6ee43-e18f-4fc0-abbf-fda96bae6b3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–±–æ—Ä–µ —Ñ–∞–π–ª–∞: no display name and no $DISPLAY environment variable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "üìà –£–õ–£–ß–®–ï–ù–ù–´–ô –ê–ù–ê–õ–ò–ó –û–ë–†–ê–©–ï–ù–ò–ô –ü–û–¢–†–ï–ë–ò–¢–ï–õ–ï–ô\n",
            "============================================================\n",
            "\n",
            "üóÇÔ∏è  –í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª Excel —Å –æ–±—Ä–∞—â–µ–Ω–∏—è–º–∏...\n",
            "\n",
            "–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–±–æ—Ä–µ —Ñ–∞–π–ª–∞: no display name and no $DISPLAY environment variable\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from openpyxl import Workbook\n",
        "from openpyxl.utils.dataframe import dataframe_to_rows\n",
        "from openpyxl.styles import Font, PatternFill, Alignment\n",
        "from openpyxl.chart import PieChart, BarChart, Reference\n",
        "import os\n",
        "import logging\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "from tkinter import Tk\n",
        "from tkinter.filedialog import askopenfilename\n",
        "import numpy as np\n",
        "import re\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è\n",
        "logging.basicConfig(\n",
        "    filename='processing.log',\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "def extract_phone_numbers(text):\n",
        "    \"\"\"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–ª–µ—Ñ–æ–Ω–Ω—ã—Ö –Ω–æ–º–µ—Ä–æ–≤\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return []\n",
        "\n",
        "    text_str = str(text)\n",
        "\n",
        "    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–Ω—ã—Ö –Ω–æ–º–µ—Ä–æ–≤\n",
        "    phone_patterns = [\n",
        "        r'\\+7\\s?\\(?\\d{3}\\)?\\s?\\d{3}[\\s-]?\\d{2}[\\s-]?\\d{2}',  # +7 —Ñ–æ—Ä–º–∞—Ç—ã\n",
        "        r'8\\s?\\(?\\d{3}\\)?\\s?\\d{3}[\\s-]?\\d{2}[\\s-]?\\d{2}',   # 8 —Ñ–æ—Ä–º–∞—Ç—ã\n",
        "        r'\\(\\d{3}\\)\\s?\\d{3}[\\s-]?\\d{2}[\\s-]?\\d{2}',         # (999) 999-99-99\n",
        "        r'\\d{3}[\\s-]?\\d{2}[\\s-]?\\d{2}[\\s-]?\\d{2}',          # 999-99-99-99\n",
        "        r'\\d{3}[\\s-]?\\d{3}[\\s-]?\\d{2}[\\s-]?\\d{2}',          # 999-999-99-99\n",
        "    ]\n",
        "\n",
        "    phones = []\n",
        "    for pattern in phone_patterns:\n",
        "        matches = re.findall(pattern, text_str)\n",
        "        for match in matches:\n",
        "            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–æ–º–µ—Ä–∞ –∫ —Ñ–æ—Ä–º–∞—Ç—É +7XXXXXXXXXX\n",
        "            clean_phone = re.sub(r'[\\s\\(\\)\\-+]', '', match)\n",
        "            if clean_phone.startswith('8') and len(clean_phone) == 11:\n",
        "                clean_phone = '+7' + clean_phone[1:]\n",
        "            elif clean_phone.startswith('7') and len(clean_phone) == 11:\n",
        "                clean_phone = '+7' + clean_phone[1:]\n",
        "            elif len(clean_phone) == 10:\n",
        "                clean_phone = '+7' + clean_phone\n",
        "\n",
        "            if len(clean_phone) == 12 and clean_phone.startswith('+7'):\n",
        "                phones.append(clean_phone)\n",
        "\n",
        "    return list(set(phones))  # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã\n",
        "\n",
        "def similar_text(text1, text2, threshold=0.85):\n",
        "    \"\"\"–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤ —Å —É—á–µ—Ç–æ–º –≤–æ–∑–º–æ–∂–Ω—ã—Ö –æ–ø–µ—á–∞—Ç–æ–∫\"\"\"\n",
        "    if pd.isna(text1) or pd.isna(text2):\n",
        "        return False\n",
        "    return SequenceMatcher(None, str(text1).lower(), str(text2).lower()).ratio() >= threshold\n",
        "\n",
        "def similar_phones(phones1, phones2):\n",
        "    \"\"\"–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å–ø–∏—Å–∫–æ–≤ —Ç–µ–ª–µ—Ñ–æ–Ω–Ω—ã—Ö –Ω–æ–º–µ—Ä–æ–≤\"\"\"\n",
        "    if not phones1 or not phones2:\n",
        "        return False\n",
        "\n",
        "    # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –Ω–æ–º–µ—Ä–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤\n",
        "    common_phones = set(phones1) & set(phones2)\n",
        "    return len(common_phones) > 0\n",
        "\n",
        "def extract_address(text):\n",
        "    \"\"\"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–¥—Ä–µ—Å–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –æ–±—Ä–∞—â–µ–Ω–∏—è\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return '–∞–¥—Ä–µ—Å –Ω–µ –Ω–∞–π–¥–µ–Ω'\n",
        "\n",
        "    text_str = str(text).lower()\n",
        "\n",
        "    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –∞–¥—Ä–µ—Å–∞\n",
        "    address_patterns = [\n",
        "        r'(—É–ª\\.?|—É–ª–∏—Ü–∞|–ø—Ä–æ—Å–ø|–ø—Ä\\.|–ø—Ä–æ—Å–ø–µ–∫—Ç|–ø–µ—Ä\\.|–ø–µ—Ä–µ—É–ª–æ–∫|—à–æ—Å—Å–µ|–Ω–∞–±\\.|–Ω–∞–±–µ—Ä–µ–∂–Ω–∞—è)[\\s\\w\\d.-]+',\n",
        "        r'–¥\\.?\\s*\\d+',\n",
        "        r'–¥–æ–º\\.?\\s*\\d+',\n",
        "        r'–∫–≤\\.?\\s*\\d+',\n",
        "        r'–∫–≤–∞—Ä—Ç–∏—Ä–∞\\.?\\s*\\d+',\n",
        "        r'–∫–æ—Ä–ø\\.?\\s*\\d+',\n",
        "        r'—Å—Ç—Ä–æ–µ–Ω\\.?\\s*\\d+',\n",
        "        r'–º–∏–∫—Ä–æ—Ä–∞–π–æ–Ω\\.?\\s*[\\w\\d]+',\n",
        "        r'—Ä-–Ω\\.?\\s*[\\w\\d]+'\n",
        "    ]\n",
        "\n",
        "    address_parts = []\n",
        "    for pattern in address_patterns:\n",
        "        matches = re.findall(pattern, text_str, re.IGNORECASE)\n",
        "        for match in matches:\n",
        "            clean_match = re.sub(r'\\s+', ' ', match).strip()\n",
        "            address_parts.append(clean_match)\n",
        "\n",
        "    unique_parts = []\n",
        "    for part in address_parts:\n",
        "        if part not in unique_parts:\n",
        "            unique_parts.append(part)\n",
        "\n",
        "    return ' '.join(unique_parts) if unique_parts else '–∞–¥—Ä–µ—Å –Ω–µ –Ω–∞–π–¥–µ–Ω'\n",
        "\n",
        "def categorize_consumer(consumer_name, appeal_text):\n",
        "    \"\"\"–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—è\"\"\"\n",
        "    if pd.isna(consumer_name):\n",
        "        consumer_name = ''\n",
        "    if pd.isna(appeal_text):\n",
        "        appeal_text = ''\n",
        "\n",
        "    consumer_str = str(consumer_name).lower()\n",
        "    appeal_str = str(appeal_text).lower()\n",
        "\n",
        "    legal_keywords = [\n",
        "        '–æ–æ–æ', '–∑–∞–æ', '–æ–∞–æ', '–∞–æ', '–ø–∞–æ', '–Ω–∞–æ', '–º–∫—É', '–º–∫—É–ø', '–º—É–ø', '–≥–±—É',\n",
        "        '–º–±—É', '–º–±–æ—É', '–º–∞–¥–æ—É', '–≥–∫–æ—É', '—É—á—Ä–µ–∂–¥–µ–Ω–∏–µ', '–ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏–µ', '–∫–æ–º–ø–∞–Ω–∏—è',\n",
        "        '—Ñ–∏—Ä–º–∞', '–∫–æ—Ä–ø—É—Å', '—Å—Ç—Ä–æ–π', '—Ä–µ–º–æ–Ω—Ç', '—Å–µ—Ä–≤–∏—Å', '—Ü–µ–Ω—Ç—Ä', '–∞–≥–µ–Ω—Ç—Å—Ç–≤–æ',\n",
        "        '—Ö–æ–ª–¥–∏–Ω–≥', '–≥—Ä—É–ø–ø', '—Ç–æ—Ä–≥', '–ø—Ä–æ–º', '–∑–∞–≤–æ–¥', '—Ñ–∞–±—Ä–∏–∫–∞', '–∫–æ–º–±–∏–Ω–∞—Ç',\n",
        "        '—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ', '–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏—è', '–º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ', '–¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç'\n",
        "    ]\n",
        "\n",
        "    for keyword in legal_keywords:\n",
        "        if keyword in consumer_str:\n",
        "            return '–Æ—Ä–∏–¥–∏—á–µ—Å–∫–æ–µ –ª–∏—Ü–æ'\n",
        "\n",
        "    for keyword in legal_keywords:\n",
        "        if keyword in appeal_str:\n",
        "            return '–Æ—Ä–∏–¥–∏—á–µ—Å–∫–æ–µ –ª–∏—Ü–æ'\n",
        "\n",
        "    if consumer_str.strip() and appeal_str.strip():\n",
        "        return '–§–∏–∑–∏—á–µ—Å–∫–æ–µ –ª–∏—Ü–æ'\n",
        "\n",
        "    return '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ'\n",
        "\n",
        "def get_file_path():\n",
        "    \"\"\"–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª—É —á–µ—Ä–µ–∑ –¥–∏–∞–ª–æ–≥–æ–≤–æ–µ –æ–∫–Ω–æ\"\"\"\n",
        "    try:\n",
        "        root = Tk()\n",
        "        root.withdraw()\n",
        "        root.attributes('-topmost', True)\n",
        "        file_path = askopenfilename(\n",
        "            title=\"–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª Excel —Å –æ–±—Ä–∞—â–µ–Ω–∏—è–º–∏\",\n",
        "            filetypes=[(\"Excel files\", \"*.xlsx *.xls\"), (\"All files\", \"*.*\")]\n",
        "        )\n",
        "        root.destroy()\n",
        "\n",
        "        if not file_path:\n",
        "            print(\"–§–∞–π–ª –Ω–µ –≤—ã–±—Ä–∞–Ω. –ü—Ä–æ–≥—Ä–∞–º–º–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.\")\n",
        "            return None\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            print(\"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.\")\n",
        "            return get_file_path()\n",
        "\n",
        "        return file_path\n",
        "    except Exception as e:\n",
        "        logging.error(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–±–æ—Ä–µ —Ñ–∞–π–ª–∞: {e}\")\n",
        "        print(f\"\\n–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–±–æ—Ä–µ —Ñ–∞–π–ª–∞: {e}\")\n",
        "        return None\n",
        "\n",
        "def load_data(file_path):\n",
        "    \"\"\"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫\"\"\"\n",
        "    try:\n",
        "        print(\"\\n–≠—Ç–∞–ø 1: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\")\n",
        "        df = pd.read_excel(file_path)\n",
        "        print(f\"–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã! –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {df.shape}\")\n",
        "        print(f\"–°—Ç–æ–ª–±—Ü—ã –≤ —Ñ–∞–π–ª–µ: {list(df.columns)}\")\n",
        "        logging.info(f\"–£—Å–ø–µ—à–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞: {file_path}\")\n",
        "        logging.info(f\"–°—Ç–æ–ª–±—Ü—ã –≤ —Ñ–∞–π–ª–µ: {list(df.columns)}\")\n",
        "\n",
        "        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–æ–ª–±—Ü–∞ —Å –¥–∞—Ç–æ–π\n",
        "        date_columns = []\n",
        "        for col in df.columns:\n",
        "            if any(keyword in str(col).lower() for keyword in ['–¥–∞—Ç–∞', 'date', '—Å–æ–∑–¥–∞–Ω', 'created']):\n",
        "                date_columns.append(col)\n",
        "\n",
        "        if date_columns:\n",
        "            print(f\"–ù–∞–π–¥–µ–Ω—ã –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã —Å –¥–∞—Ç–æ–π: {date_columns}\")\n",
        "            # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –ø–æ–¥—Ö–æ–¥—è—â–∏–π —Å—Ç–æ–ª–±–µ—Ü\n",
        "            date_column = date_columns[0]\n",
        "            df[date_column] = pd.to_datetime(df[date_column], errors='coerce')\n",
        "            print(f\"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–æ–ª–±–µ—Ü —Å –¥–∞—Ç–æ–π: '{date_column}'\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è –°—Ç–æ–ª–±–µ—Ü —Å –¥–∞—Ç–æ–π –Ω–µ –Ω–∞–π–¥–µ–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏\")\n",
        "\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        logging.error(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}\")\n",
        "        print(f\"\\n–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ñ–∞–π–ª–∞: {e}\")\n",
        "        return None\n",
        "\n",
        "def validate_data(df):\n",
        "    \"\"\"–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö\"\"\"\n",
        "    try:\n",
        "        print(\"\\n–≠—Ç–∞–ø 2: –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\")\n",
        "\n",
        "        # –í—ã–≤–æ–¥–∏–º –≤—Å–µ —Å—Ç–æ–ª–±—Ü—ã –¥–ª—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏\n",
        "        print(\"–°—Ç–æ–ª–±—Ü—ã –≤ –¥–∞–Ω–Ω—ã—Ö:\")\n",
        "        for i, col in enumerate(df.columns, 1):\n",
        "            print(f\"  {i}. {col}\")\n",
        "\n",
        "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤\n",
        "        required_keywords = {\n",
        "            '–ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å': ['–ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å', '–∫–ª–∏–µ–Ω—Ç', '–∑–∞—è–≤–∏—Ç–µ–ª—å'],\n",
        "            '—Ç–µ–∫—Å—Ç –æ–±—Ä–∞—â–µ–Ω–∏—è': ['–æ–±—Ä–∞—â–µ–Ω', '—Ç–µ–∫—Å—Ç', '–æ–ø–∏—Å–∞–Ω', '–∂–∞–ª–æ–±'],\n",
        "            '–∞–¥—Ä–µ—Å': ['–∞–¥—Ä–µ—Å', '–º–µ—Å—Ç–æ–ø–æ–ª–æ–∂', '–æ–±—ä–µ–∫—Ç']\n",
        "        }\n",
        "\n",
        "        found_columns = {}\n",
        "        for key, keywords in required_keywords.items():\n",
        "            for col in df.columns:\n",
        "                if any(keyword in str(col).lower() for keyword in keywords):\n",
        "                    found_columns[key] = col\n",
        "                    break\n",
        "\n",
        "        print(\"\\n–ù–∞–π–¥–µ–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã:\")\n",
        "        for key, col in found_columns.items():\n",
        "            print(f\"  {key}: '{col}'\")\n",
        "\n",
        "        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è\n",
        "        key_columns = list(found_columns.values())\n",
        "        if key_columns:\n",
        "            empty_count = df[key_columns].isnull().sum()\n",
        "            if empty_count.any():\n",
        "                print(f\"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ù–∞–π–¥–µ–Ω—ã –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è:\\n{empty_count}\")\n",
        "\n",
        "        print(\"–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!\")\n",
        "        logging.info(\"–£—Å–ø–µ—à–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö\")\n",
        "        return df, found_columns\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö: {e}\")\n",
        "        print(f\"\\n–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def find_date_column(df):\n",
        "    \"\"\"–ü–æ–∏—Å–∫ —Å—Ç–æ–ª–±—Ü–∞ —Å –¥–∞—Ç–æ–π\"\"\"\n",
        "    for col in df.columns:\n",
        "        if any(keyword in str(col).lower() for keyword in ['–¥–∞—Ç–∞', 'date', '—Å–æ–∑–¥–∞–Ω', 'created']):\n",
        "            return col\n",
        "    return None\n",
        "\n",
        "def find_unique_and_duplicates(df, column_mapping):\n",
        "    \"\"\"–£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ –æ–±—Ä–∞—â–µ–Ω–∏–π —Å —É—á–µ—Ç–æ–º —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤ –∏ –¥–∞—Ç\"\"\"\n",
        "    try:\n",
        "        print(\"\\n–≠—Ç–∞–ø 3: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏...\")\n",
        "\n",
        "        # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è —Å—Ç–æ–ª–±—Ü–æ–≤ –∏–∑ mapping\n",
        "        consumer_col = column_mapping.get('–ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å', '–ü–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å')\n",
        "        appeal_col = column_mapping.get('—Ç–µ–∫—Å—Ç –æ–±—Ä–∞—â–µ–Ω–∏—è', '–¢–µ–∫—Å—Ç –æ–±—Ä–∞—â–µ–Ω–∏—è')\n",
        "        address_col = column_mapping.get('–∞–¥—Ä–µ—Å', '–ê–¥—Ä–µ—Å')\n",
        "\n",
        "        # –ù–∞—Ö–æ–¥–∏–º —Å—Ç–æ–ª–±–µ—Ü —Å –¥–∞—Ç–æ–π\n",
        "        date_col = find_date_column(df)\n",
        "        if not date_col:\n",
        "            print(\"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω —Å—Ç–æ–ª–±–µ—Ü —Å –¥–∞—Ç–æ–π! –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–∞.\")\n",
        "            return df, pd.DataFrame(), df\n",
        "\n",
        "        print(f\"–ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —Å—Ç–æ–ª–±—Ü—ã:\")\n",
        "        print(f\"  –ü–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å: '{consumer_col}'\")\n",
        "        print(f\"  –¢–µ–∫—Å—Ç –æ–±—Ä–∞—â–µ–Ω–∏—è: '{appeal_col}'\")\n",
        "        print(f\"  –ê–¥—Ä–µ—Å: '{address_col}'\")\n",
        "        print(f\"  –î–∞—Ç–∞: '{date_col}'\")\n",
        "\n",
        "        # –°–æ–∑–¥–∞–µ–º —Ä–∞–±–æ—á—É—é –∫–æ–ø–∏—é\n",
        "        work_df = df.copy()\n",
        "\n",
        "        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∞–¥—Ä–µ—Å–∞ –∏ —Ç–µ–ª–µ—Ñ–æ–Ω—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞ –æ–±—Ä–∞—â–µ–Ω–∏–π\n",
        "        print(\"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–¥—Ä–µ—Å–æ–≤ –∏ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –æ–±—Ä–∞—â–µ–Ω–∏–π...\")\n",
        "        work_df['–ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π –∞–¥—Ä–µ—Å'] = work_df[appeal_col].apply(extract_address)\n",
        "        work_df['–¢–µ–ª–µ—Ñ–æ–Ω—ã'] = work_df[appeal_col].apply(extract_phone_numbers)\n",
        "        work_df['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤'] = work_df['–¢–µ–ª–µ—Ñ–æ–Ω—ã'].apply(len)\n",
        "\n",
        "        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
        "        work_df['–¢–µ–∫—Å—Ç_normalized'] = work_df[appeal_col].astype(str).str.lower().str.strip()\n",
        "        work_df['–ü–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å_normalized'] = work_df[consumer_col].astype(str).str.lower().str.strip()\n",
        "        work_df['–î–∞—Ç–∞_–¥–µ–Ω—å'] = work_df[date_col].dt.date\n",
        "        work_df['–î–∞—Ç–∞_–Ω–µ–¥–µ–ª—è'] = work_df[date_col].dt.to_period('W').apply(lambda r: r.start_time.date())\n",
        "        work_df['–î–∞—Ç–∞_–º–µ—Å—è—Ü'] = work_df[date_col].dt.to_period('M').apply(lambda r: r.start_time.date())\n",
        "\n",
        "        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—è\n",
        "        work_df['–ö–∞—Ç–µ–≥–æ—Ä–∏—è –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—è'] = work_df.apply(\n",
        "            lambda row: categorize_consumer(row[consumer_col], row[appeal_col]), axis=1\n",
        "        )\n",
        "\n",
        "        # –°–æ–∑–¥–∞–µ–º –∫–æ–ª–æ–Ω–∫—É –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏\n",
        "        work_df['group_key'] = work_df['–ü–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å_normalized'] + '|' + work_df['–î–∞—Ç–∞_–Ω–µ–¥–µ–ª—è'].astype(str)\n",
        "\n",
        "        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–∞–ø–∏—Å–∏\n",
        "        work_df['–¢–∏–ø –∑–∞–ø–∏—Å–∏'] = '–£–Ω–∏–∫–∞–ª—å–Ω–∞—è'\n",
        "        work_df['–ì—Ä—É–ø–ø–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤'] = ''\n",
        "        work_df['–ö—Ä–∏—Ç–µ—Ä–∏–π —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è'] = ''\n",
        "\n",
        "        print(\"–ü–æ–∏—Å–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å —É—á–µ—Ç–æ–º —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤, —Ç–µ–∫—Å—Ç–∞ –∏ –¥–∞—Ç...\")\n",
        "\n",
        "        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—é –∏ –Ω–µ–¥–µ–ª–µ\n",
        "        group_keys = work_df['group_key'].unique()\n",
        "        total_groups = len(group_keys)\n",
        "\n",
        "        for i, group_key in enumerate(group_keys):\n",
        "            group = work_df[work_df['group_key'] == group_key]\n",
        "\n",
        "            if len(group) > 1:\n",
        "                if (i + 1) % 100 == 0:\n",
        "                    print(f\"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –≥—Ä—É–ø–ø: {i + 1}/{total_groups}\")\n",
        "\n",
        "                group_indices = group.index.tolist()\n",
        "\n",
        "                for i_idx in range(len(group_indices)):\n",
        "                    idx_i = group_indices[i_idx]\n",
        "                    if work_df.loc[idx_i, '–¢–∏–ø –∑–∞–ø–∏—Å–∏'] == '–î—É–±–ª–∏–∫–∞—Ç':\n",
        "                        continue\n",
        "\n",
        "                    text_i = work_df.loc[idx_i, '–¢–µ–∫—Å—Ç_normalized']\n",
        "                    address_i = work_df.loc[idx_i, '–ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π –∞–¥—Ä–µ—Å']\n",
        "                    phones_i = work_df.loc[idx_i, '–¢–µ–ª–µ—Ñ–æ–Ω—ã']\n",
        "                    date_i = work_df.loc[idx_i, '–î–∞—Ç–∞_–¥–µ–Ω—å']\n",
        "\n",
        "                    for j_idx in range(i_idx + 1, len(group_indices)):\n",
        "                        idx_j = group_indices[j_idx]\n",
        "                        if work_df.loc[idx_j, '–¢–∏–ø –∑–∞–ø–∏—Å–∏'] == '–î—É–±–ª–∏–∫–∞—Ç':\n",
        "                            continue\n",
        "\n",
        "                        text_j = work_df.loc[idx_j, '–¢–µ–∫—Å—Ç_normalized']\n",
        "                        address_j = work_df.loc[idx_j, '–ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π –∞–¥—Ä–µ—Å']\n",
        "                        phones_j = work_df.loc[idx_j, '–¢–µ–ª–µ—Ñ–æ–Ω—ã']\n",
        "                        date_j = work_df.loc[idx_j, '–î–∞—Ç–∞_–¥–µ–Ω—å']\n",
        "\n",
        "                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è\n",
        "                        match_criteria = []\n",
        "\n",
        "                        # 1. –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤ (—Å–∞–º—ã–π —Å–∏–ª—å–Ω—ã–π –∫—Ä–∏—Ç–µ—Ä–∏–π)\n",
        "                        if similar_phones(phones_i, phones_j):\n",
        "                            match_criteria.append('—Ç–µ–ª–µ—Ñ–æ–Ω—ã')\n",
        "\n",
        "                        # 2. –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞\n",
        "                        if similar_text(text_i, text_j):\n",
        "                            match_criteria.append('—Ç–µ–∫—Å—Ç')\n",
        "\n",
        "                        # 3. –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∞–¥—Ä–µ—Å–∞\n",
        "                        if similar_text(address_i, address_j, threshold=0.7):\n",
        "                            match_criteria.append('–∞–¥—Ä–µ—Å')\n",
        "\n",
        "                        # 4. –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –¥–∞—Ç—ã (–≤ –ø—Ä–µ–¥–µ–ª–∞—Ö 3 –¥–Ω–µ–π)\n",
        "                        date_diff = abs((date_i - date_j).days)\n",
        "                        if date_diff <= 3:\n",
        "                            match_criteria.append('–¥–∞—Ç–∞')\n",
        "\n",
        "                        # –ï—Å–ª–∏ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã 2 —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –∏–ª–∏ —Ç–µ–ª–µ—Ñ–æ–Ω—ã + —á—Ç–æ-—Ç–æ –µ—â–µ\n",
        "                        if (len(match_criteria) >= 2 or\n",
        "                            ('—Ç–µ–ª–µ—Ñ–æ–Ω—ã' in match_criteria and len(match_criteria) >= 1)):\n",
        "\n",
        "                            if work_df.loc[idx_i, '–¢–∏–ø –∑–∞–ø–∏—Å–∏'] == '–£–Ω–∏–∫–∞–ª—å–Ω–∞—è':\n",
        "                                work_df.loc[idx_i, '–¢–∏–ø –∑–∞–ø–∏—Å–∏'] = '–û—Å–Ω–æ–≤–Ω–æ–π –≤ –≥—Ä—É–ø–ø–µ'\n",
        "                                work_df.loc[idx_i, '–ì—Ä—É–ø–ø–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤'] = f\"–ì—Ä—É–ø–ø–∞_{idx_i}\"\n",
        "                                work_df.loc[idx_i, '–ö—Ä–∏—Ç–µ—Ä–∏–π —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è'] = ', '.join(match_criteria)\n",
        "\n",
        "                            work_df.loc[idx_j, '–¢–∏–ø –∑–∞–ø–∏—Å–∏'] = '–î—É–±–ª–∏–∫–∞—Ç'\n",
        "                            work_df.loc[idx_j, '–ì—Ä—É–ø–ø–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤'] = f\"–ì—Ä—É–ø–ø–∞_{idx_i}\"\n",
        "                            work_df.loc[idx_j, '–ö—Ä–∏—Ç–µ—Ä–∏–π —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è'] = ', '.join(match_criteria)\n",
        "\n",
        "        # –°–æ–∑–¥–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ DataFrames\n",
        "        unique_df = work_df[work_df['–¢–∏–ø –∑–∞–ø–∏—Å–∏'].isin(['–£–Ω–∏–∫–∞–ª—å–Ω–∞—è', '–û—Å–Ω–æ–≤–Ω–æ–π –≤ –≥—Ä—É–ø–ø–µ'])].copy()\n",
        "        duplicates_detailed = work_df[work_df['–¢–∏–ø –∑–∞–ø–∏—Å–∏'] == '–î—É–±–ª–∏–∫–∞—Ç'].copy()\n",
        "\n",
        "        # –£–±–∏—Ä–∞–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –∏–∑ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü\n",
        "        columns_to_drop = ['–¢–µ–∫—Å—Ç_normalized', '–ü–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å_normalized', '–î–∞—Ç–∞_–¥–µ–Ω—å',\n",
        "                          '–î–∞—Ç–∞_–Ω–µ–¥–µ–ª—è', '–î–∞—Ç–∞_–º–µ—Å—è—Ü', 'group_key', '–ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π –∞–¥—Ä–µ—Å', '–¢–µ–ª–µ—Ñ–æ–Ω—ã']\n",
        "\n",
        "        final_columns = [col for col in unique_df.columns if col not in columns_to_drop]\n",
        "        unique_df = unique_df[final_columns]\n",
        "        duplicates_detailed = duplicates_detailed[final_columns]\n",
        "\n",
        "        print(f\"‚úÖ –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –æ–±—Ä–∞—â–µ–Ω–∏—è: {len(unique_df)}\")\n",
        "        print(f\"‚úÖ –î—É–±–ª–∏–∫–∞—Ç—ã: {len(duplicates_detailed)}\")\n",
        "        print(f\"‚úÖ –ò–∑ –Ω–∏—Ö –æ—Å–Ω–æ–≤–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π: {len(unique_df[unique_df['–¢–∏–ø –∑–∞–ø–∏—Å–∏'] == '–û—Å–Ω–æ–≤–Ω–æ–π –≤ –≥—Ä—É–ø–ø–µ'])}\")\n",
        "\n",
        "        logging.info(f\"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –æ–±—Ä–∞—â–µ–Ω–∏–π: {len(unique_df)}, –î—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(duplicates_detailed)}\")\n",
        "        return unique_df, duplicates_detailed, work_df\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏: {e}\")\n",
        "        print(f\"\\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}\")\n",
        "        return df, pd.DataFrame(), df\n",
        "\n",
        "def create_detailed_statistics(original_df, unique_df, duplicates_detailed, work_df):\n",
        "    \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å –≥—Ä–∞—Ñ–∏–∫–∞–º–∏\"\"\"\n",
        "    try:\n",
        "        print(\"\\n–°–æ–∑–¥–∞–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏...\")\n",
        "\n",
        "        stats_data = []\n",
        "\n",
        "        # –û—Å–Ω–æ–≤–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
        "        total_appeals = len(original_df)\n",
        "        total_unique = len(unique_df)\n",
        "        total_duplicates = len(duplicates_detailed)\n",
        "\n",
        "        stats_data.append({'–ö–∞—Ç–µ–≥–æ—Ä–∏—è': '–û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê', '–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å': '–í—Å–µ–≥–æ –æ–±—Ä–∞—â–µ–Ω–∏–π', '–ó–Ω–∞—á–µ–Ω–∏–µ': total_appeals})\n",
        "        stats_data.append({'–ö–∞—Ç–µ–≥–æ—Ä–∏—è': '–û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê', '–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å': '–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –æ–±—Ä–∞—â–µ–Ω–∏–π', '–ó–Ω–∞—á–µ–Ω–∏–µ': total_unique})\n",
        "        stats_data.append({'–ö–∞—Ç–µ–≥–æ—Ä–∏—è': '–û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê', '–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å': '–î—É–±–ª–∏–∫–∞—Ç–æ–≤', '–ó–Ω–∞—á–µ–Ω–∏–µ': total_duplicates})\n",
        "        stats_data.append({'–ö–∞—Ç–µ–≥–æ—Ä–∏—è': '–û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê', '–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å': '–ü—Ä–æ—Ü–µ–Ω—Ç –¥—É–±–ª–∏–∫–∞—Ç–æ–≤',\n",
        "                          '–ó–Ω–∞—á–µ–Ω–∏–µ': f\"{(total_duplicates/total_appeals*100):.1f}%\" if total_appeals > 0 else '0%'})\n",
        "\n",
        "        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª–µ–π\n",
        "        if '–ö–∞—Ç–µ–≥–æ—Ä–∏—è –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—è' in work_df.columns:\n",
        "            category_stats = work_df['–ö–∞—Ç–µ–≥–æ—Ä–∏—è –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—è'].value_counts()\n",
        "            for category, count in category_stats.items():\n",
        "                stats_data.append({\n",
        "                    '–ö–∞—Ç–µ–≥–æ—Ä–∏—è': '–ö–ê–¢–ï–ì–û–†–ò–ò –ü–û–¢–†–ï–ë–ò–¢–ï–õ–ï–ô',\n",
        "                    '–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å': category,\n",
        "                    '–ó–Ω–∞—á–µ–Ω–∏–µ': count\n",
        "                })\n",
        "\n",
        "        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º –∑–∞–ø–∏—Å–µ–π\n",
        "        if '–¢–∏–ø –∑–∞–ø–∏—Å–∏' in work_df.columns:\n",
        "            type_stats = work_df['–¢–∏–ø –∑–∞–ø–∏—Å–∏'].value_counts()\n",
        "            for type_name, count in type_stats.items():\n",
        "                stats_data.append({\n",
        "                    '–ö–∞—Ç–µ–≥–æ—Ä–∏—è': '–¢–ò–ü–´ –ó–ê–ü–ò–°–ï–ô',\n",
        "                    '–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å': type_name,\n",
        "                    '–ó–Ω–∞—á–µ–Ω–∏–µ': count\n",
        "                })\n",
        "\n",
        "        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è\n",
        "        if '–ö—Ä–∏—Ç–µ—Ä–∏–π —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è' in duplicates_detailed.columns:\n",
        "            criteria_stats = duplicates_detailed['–ö—Ä–∏—Ç–µ—Ä–∏–π —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è'].value_counts().head(10)\n",
        "            for criteria, count in criteria_stats.items():\n",
        "                if criteria:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ\n",
        "                    stats_data.append({\n",
        "                        '–ö–∞—Ç–µ–≥–æ—Ä–∏—è': '–ö–†–ò–¢–ï–†–ò–ò –°–û–í–ü–ê–î–ï–ù–ò–Ø –î–£–ë–õ–ò–ö–ê–¢–û–í',\n",
        "                        '–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å': criteria,\n",
        "                        '–ó–Ω–∞—á–µ–Ω–∏–µ': count\n",
        "                    })\n",
        "\n",
        "        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–∞—Ç–∞–º\n",
        "        date_col = find_date_column(original_df)\n",
        "        if date_col:\n",
        "            # –ü–æ –º–µ—Å—è—Ü–∞–º\n",
        "            original_df['–ú–µ—Å—è—Ü'] = original_df[date_col].dt.to_period('M')\n",
        "            monthly_stats = original_df['–ú–µ—Å—è—Ü'].value_counts().sort_index()\n",
        "            for month, count in monthly_stats.items():\n",
        "                stats_data.append({\n",
        "                    '–ö–∞—Ç–µ–≥–æ—Ä–∏—è': '–°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –ú–ï–°–Ø–¶–ê–ú',\n",
        "                    '–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å': f'{month}',\n",
        "                    '–ó–Ω–∞—á–µ–Ω–∏–µ': count\n",
        "                })\n",
        "\n",
        "            # –ü–æ –¥–Ω—è–º –Ω–µ–¥–µ–ª–∏\n",
        "            original_df['–î–µ–Ω—å –Ω–µ–¥–µ–ª–∏'] = original_df[date_col].dt.day_name()\n",
        "            day_stats = original_df['–î–µ–Ω—å –Ω–µ–¥–µ–ª–∏'].value_counts()\n",
        "            for day, count in day_stats.items():\n",
        "                stats_data.append({\n",
        "                    '–ö–∞—Ç–µ–≥–æ—Ä–∏—è': '–°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –î–ù–Ø–ú –ù–ï–î–ï–õ–ò',\n",
        "                    '–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å': day,\n",
        "                    '–ó–Ω–∞—á–µ–Ω–∏–µ': count\n",
        "                })\n",
        "\n",
        "        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–µ–ª–µ—Ñ–æ–Ω–∞–º\n",
        "        if '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤' in work_df.columns:\n",
        "            phone_stats = work_df['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤'].value_counts().sort_index()\n",
        "            for count_phones, records in phone_stats.items():\n",
        "                stats_data.append({\n",
        "                    '–ö–∞—Ç–µ–≥–æ—Ä–∏—è': '–°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –¢–ï–õ–ï–§–û–ù–ê–ú',\n",
        "                    '–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å': f'–û–±—Ä–∞—â–µ–Ω–∏–π —Å {count_phones} —Ç–µ–ª–µ—Ñ–æ–Ω–æ–º(–∞–º–∏)',\n",
        "                    '–ó–Ω–∞—á–µ–Ω–∏–µ': records\n",
        "                })\n",
        "\n",
        "        stats_df = pd.DataFrame(stats_data)\n",
        "        return stats_df\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}\")\n",
        "        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def create_analysis_charts(work_df, writer):\n",
        "    \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞\"\"\"\n",
        "    try:\n",
        "        workbook = writer.book\n",
        "        worksheet = workbook.create_sheet('–ì—Ä–∞—Ñ–∏–∫–∏ –∞–Ω–∞–ª–∏–∑–∞')\n",
        "\n",
        "        # –ó–∞–≥–æ–ª–æ–≤–æ–∫\n",
        "        worksheet['A1'] = '–ê–ù–ê–õ–ò–¢–ò–ß–ï–°–ö–ò–ï –ì–†–ê–§–ò–ö–ò'\n",
        "        worksheet['A1'].font = Font(bold=True, size=16)\n",
        "\n",
        "        current_row = 3\n",
        "\n",
        "        # 1. –ì—Ä–∞—Ñ–∏–∫ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª–µ–π\n",
        "        if '–ö–∞—Ç–µ–≥–æ—Ä–∏—è –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—è' in work_df.columns:\n",
        "            category_stats = work_df['–ö–∞—Ç–µ–≥–æ—Ä–∏—è –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—è'].value_counts()\n",
        "\n",
        "            worksheet.cell(row=current_row, column=1, value='–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª–µ–π')\n",
        "            current_row += 1\n",
        "\n",
        "            for i, (category, count) in enumerate(category_stats.items(), 1):\n",
        "                worksheet.cell(row=current_row, column=1, value=category)\n",
        "                worksheet.cell(row=current_row, column=2, value=count)\n",
        "                current_row += 1\n",
        "\n",
        "            current_row += 2\n",
        "\n",
        "        # 2. –ì—Ä–∞—Ñ–∏–∫ –ø–æ —Ç–∏–ø–∞–º –∑–∞–ø–∏—Å–µ–π\n",
        "        if '–¢–∏–ø –∑–∞–ø–∏—Å–∏' in work_df.columns:\n",
        "            type_stats = work_df['–¢–∏–ø –∑–∞–ø–∏—Å–∏'].value_counts()\n",
        "\n",
        "            worksheet.cell(row=current_row, column=1, value='–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ç–∏–ø–∞–º –∑–∞–ø–∏—Å–µ–π')\n",
        "            current_row += 1\n",
        "\n",
        "            for i, (type_name, count) in enumerate(type_stats.items(), 1):\n",
        "                worksheet.cell(row=current_row, column=1, value=type_name)\n",
        "                worksheet.cell(row=current_row, column=2, value=count)\n",
        "                current_row += 1\n",
        "\n",
        "            current_row += 2\n",
        "\n",
        "        # 3. –ì—Ä–∞—Ñ–∏–∫ –ø–æ –º–µ—Å—è—Ü–∞–º\n",
        "        date_col = find_date_column(work_df)\n",
        "        if date_col:\n",
        "            work_df['–ú–µ—Å—è—Ü'] = work_df[date_col].dt.strftime('%Y-%m')\n",
        "            monthly_stats = work_df['–ú–µ—Å—è—Ü'].value_counts().sort_index()\n",
        "\n",
        "            worksheet.cell(row=current_row, column=1, value='–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –º–µ—Å—è—Ü–∞–º')\n",
        "            current_row += 1\n",
        "\n",
        "            for i, (month, count) in enumerate(monthly_stats.items(), 1):\n",
        "                worksheet.cell(row=current_row, column=1, value=month)\n",
        "                worksheet.cell(row=current_row, column=2, value=count)\n",
        "                current_row += 1\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –≥—Ä–∞—Ñ–∏–∫–æ–≤: {e}\")\n",
        "        print(f\"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –≥—Ä–∞—Ñ–∏–∫–∏: {e}\")\n",
        "        return False\n",
        "\n",
        "def format_excel_report(writer):\n",
        "    \"\"\"–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Excel –æ—Ç—á–µ—Ç–∞\"\"\"\n",
        "    try:\n",
        "        workbook = writer.book\n",
        "\n",
        "        header_font = Font(bold=True, color=\"FFFFFF\")\n",
        "        header_fill = PatternFill(start_color=\"366092\", end_color=\"366092\", fill_type=\"solid\")\n",
        "        center_align = Alignment(horizontal=\"center\", vertical=\"center\")\n",
        "\n",
        "        for sheet_name in workbook.sheetnames:\n",
        "            worksheet = workbook[sheet_name]\n",
        "\n",
        "            # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤\n",
        "            for cell in worksheet[1]:\n",
        "                cell.font = header_font\n",
        "                cell.fill = header_fill\n",
        "                cell.alignment = center_align\n",
        "\n",
        "            # –ê–≤—Ç–æ–ø–æ–¥–±–æ—Ä —à–∏—Ä–∏–Ω—ã —Å—Ç–æ–ª–±—Ü–æ–≤\n",
        "            for column in worksheet.columns:\n",
        "                max_length = 0\n",
        "                column_letter = column[0].column_letter\n",
        "                for cell in column:\n",
        "                    try:\n",
        "                        if len(str(cell.value)) > max_length:\n",
        "                            max_length = len(str(cell.value))\n",
        "                    except:\n",
        "                        pass\n",
        "                adjusted_width = min(max_length + 2, 50)\n",
        "                worksheet.column_dimensions[column_letter].width = adjusted_width\n",
        "\n",
        "            # –î–æ–±–∞–≤–ª—è–µ–º –∞–≤—Ç–æ—Ñ–∏–ª—å—Ç—Ä –Ω–∞ –ª–∏—Å—Ç –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
        "            if sheet_name == '–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞':\n",
        "                worksheet.auto_filter.ref = worksheet.dimensions\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ Excel: {e}\")\n",
        "\n",
        "def generate_report(original_df, unique_df, duplicates_detailed, work_df, column_mapping):\n",
        "    \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π\"\"\"\n",
        "    try:\n",
        "        print(\"\\n–≠—Ç–∞–ø 4: –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞...\")\n",
        "\n",
        "        output_path = Path('–æ—Ç—á–µ—Ç_–æ–±—Ä–∞—â–µ–Ω–∏–π.xlsx')\n",
        "\n",
        "        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
        "            # 1. –õ–∏—Å—Ç: –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
        "            original_df.to_excel(writer, sheet_name='–ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ', index=False)\n",
        "            print(\"‚úì –õ–∏—Å—Ç '–ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ' —Å–æ–∑–¥–∞–Ω\")\n",
        "\n",
        "            # 2. –õ–∏—Å—Ç: –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –æ–±—Ä–∞—â–µ–Ω–∏—è\n",
        "            if len(unique_df) > 0:\n",
        "                unique_df.to_excel(writer, sheet_name='–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ', index=False)\n",
        "                print(\"‚úì –õ–∏—Å—Ç '–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ' —Å–æ–∑–¥–∞–Ω\")\n",
        "            else:\n",
        "                pd.DataFrame({'–°–æ–æ–±—â–µ–Ω–∏–µ': ['–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –æ–±—Ä–∞—â–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã']}).to_excel(\n",
        "                    writer, sheet_name='–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ', index=False\n",
        "                )\n",
        "\n",
        "            # 3. –õ–∏—Å—Ç: –î—É–±–ª–∏–∫–∞—Ç—ã\n",
        "            if len(duplicates_detailed) > 0:\n",
        "                duplicates_detailed.to_excel(writer, sheet_name='–î—É–±–ª–∏–∫–∞—Ç—ã', index=False)\n",
        "                print(\"‚úì –õ–∏—Å—Ç '–î—É–±–ª–∏–∫–∞—Ç—ã' —Å–æ–∑–¥–∞–Ω\")\n",
        "            else:\n",
        "                pd.DataFrame({'–°–æ–æ–±—â–µ–Ω–∏–µ': ['–î—É–±–ª–∏–∫–∞—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã']}).to_excel(\n",
        "                    writer, sheet_name='–î—É–±–ª–∏–∫–∞—Ç—ã', index=False\n",
        "                )\n",
        "\n",
        "            # 4. –õ–∏—Å—Ç: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
        "            stats_df = create_detailed_statistics(original_df, unique_df, duplicates_detailed, work_df)\n",
        "            stats_df.to_excel(writer, sheet_name='–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞', index=False)\n",
        "            print(\"‚úì –õ–∏—Å—Ç '–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞' —Å–æ–∑–¥–∞–Ω\")\n",
        "\n",
        "            # 5. –õ–∏—Å—Ç: –ì—Ä–∞—Ñ–∏–∫–∏ –∞–Ω–∞–ª–∏–∑–∞\n",
        "            create_analysis_charts(work_df, writer)\n",
        "            print(\"‚úì –õ–∏—Å—Ç '–ì—Ä–∞—Ñ–∏–∫–∏ –∞–Ω–∞–ª–∏–∑–∞' —Å–æ–∑–¥–∞–Ω\")\n",
        "\n",
        "            format_excel_report(writer)\n",
        "\n",
        "        print(f\"\\nüìä –û—Ç—á–µ—Ç —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –ø–æ –ø—É—Ç–∏: {output_path.absolute()}\")\n",
        "        print(f\"üìã –õ–∏—Å—Ç—ã –æ—Ç—á–µ—Ç–∞:\")\n",
        "        print(f\"   1. –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ - {len(original_df)} –∑–∞–ø–∏—Å–µ–π\")\n",
        "        print(f\"   2. –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ - {len(unique_df)} –∑–∞–ø–∏—Å–µ–π\")\n",
        "        print(f\"   3. –î—É–±–ª–∏–∫–∞—Ç—ã - {len(duplicates_detailed)} –∑–∞–ø–∏—Å–µ–π\")\n",
        "        print(f\"   4. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ - –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è\")\n",
        "        print(f\"   5. –ì—Ä–∞—Ñ–∏–∫–∏ –∞–Ω–∞–ª–∏–∑–∞ - –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö\")\n",
        "\n",
        "        logging.info(f\"–£—Å–ø–µ—à–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞: {output_path}\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –æ—Ç—á–µ—Ç–∞: {e}\")\n",
        "        print(f\"\\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–∏ –æ—Ç—á–µ—Ç–∞: {e}\")\n",
        "        return False\n",
        "\n",
        "def main():\n",
        "    \"\"\"–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–æ–≥—Ä–∞–º–º—ã\"\"\"\n",
        "    try:\n",
        "        print(\"=\" * 60)\n",
        "        print(\"üìà –£–õ–£–ß–®–ï–ù–ù–´–ô –ê–ù–ê–õ–ò–ó –û–ë–†–ê–©–ï–ù–ò–ô –ü–û–¢–†–ï–ë–ò–¢–ï–õ–ï–ô\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        print(\"\\nüóÇÔ∏è  –í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª Excel —Å –æ–±—Ä–∞—â–µ–Ω–∏—è–º–∏...\")\n",
        "        file_path = get_file_path()\n",
        "\n",
        "        if not file_path:\n",
        "            return\n",
        "\n",
        "        df = load_data(file_path)\n",
        "        if df is None:\n",
        "            print(\"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ. –ü—Ä–æ–≥—Ä–∞–º–º–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.\")\n",
        "            return\n",
        "\n",
        "        df, column_mapping = validate_data(df)\n",
        "        if df is None:\n",
        "            print(\"‚ùå –î–∞–Ω–Ω—ã–µ –Ω–µ –ø—Ä–æ—à–ª–∏ –ø—Ä–æ–≤–µ—Ä–∫—É. –ü—Ä–æ–≥—Ä–∞–º–º–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.\")\n",
        "            return\n",
        "\n",
        "        unique_df, duplicates_detailed, work_df = find_unique_and_duplicates(df, column_mapping)\n",
        "\n",
        "        success = generate_report(df, unique_df, duplicates_detailed, work_df, column_mapping)\n",
        "\n",
        "        if success:\n",
        "            print(\"\\n\" + \"=\" * 60)\n",
        "            print(\"‚úÖ –û–ë–†–ê–ë–û–¢–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê –£–°–ü–ï–®–ù–û!\")\n",
        "            print(\"=\" * 60)\n",
        "            print(f\"üìà –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:\")\n",
        "            print(f\"   ‚Ä¢ –ò—Å—Ö–æ–¥–Ω—ã–µ –∑–∞–ø–∏—Å–∏: {len(df)}\")\n",
        "            print(f\"   ‚Ä¢ –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–∞–ø–∏—Å–∏: {len(unique_df)}\")\n",
        "            print(f\"   ‚Ä¢ –î—É–±–ª–∏–∫–∞—Ç—ã: {len(duplicates_detailed)}\")\n",
        "\n",
        "            try:\n",
        "                if os.name == 'nt':\n",
        "                    os.startfile('–æ—Ç—á–µ—Ç_–æ–±—Ä–∞—â–µ–Ω–∏–π.xlsx')\n",
        "                else:\n",
        "                    subprocess.call(['open', '–æ—Ç—á–µ—Ç_–æ–±—Ä–∞—â–µ–Ω–∏–π.xlsx'])\n",
        "            except:\n",
        "                print(\"‚ö†Ô∏è  –§–∞–π–ª —Å–æ–∑–¥–∞–Ω, –Ω–æ –Ω–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏\")\n",
        "\n",
        "        else:\n",
        "            print(\"\\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –æ—Ç—á–µ—Ç–∞.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ main: {e}\")\n",
        "        print(f\"\\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ba3b029",
        "outputId": "bd5da740-ceb5-457f-db46-8811c1ca3164"
      },
      "source": [
        "%pip install streamlit"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.12/dist-packages (1.50.0)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.5.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.8.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.27.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d921fd7",
        "outputId": "b353237a-831e-4520-c360-f41a6aa426bc"
      },
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import os\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import logging\n",
        "from datetime import datetime\n",
        "\n",
        "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è (–º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç—É –∂–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–¥–∞)\n",
        "logging.basicConfig(\n",
        "    filename='processing.log',\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "# –ò–º–ø–æ—Ä—Ç —Ñ—É–Ω–∫—Ü–∏–π –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–π —è—á–µ–π–∫–∏ (–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ –ø—Ä–µ–¥—ã–¥—É—â–∞—è —è—á–µ–π–∫–∞ –±—ã–ª–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞)\n",
        "# from __main__ import load_data, validate_data, find_unique_and_duplicates, generate_report, categorize_consumer, extract_phone_numbers, similar_text, similar_phones, extract_address, find_date_column, create_detailed_statistics, create_analysis_charts, format_excel_report\n",
        "\n",
        "# –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç\n",
        "UPLOAD_FOLDER = Path(\"process_data\")\n",
        "UPLOAD_FOLDER.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "st.title(\"üìà –ê–Ω–∞–ª–∏–∑ –æ–±—Ä–∞—â–µ–Ω–∏–π –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª–µ–π\")\n",
        "st.write(\"–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª Excel –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏ –≤—ã—è–≤–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª Excel\", type=[\"xlsx\", \"xls\"])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π —Ñ–∞–π–ª\n",
        "    file_path = UPLOAD_FOLDER / uploaded_file.name\n",
        "    with open(file_path, \"wb\") as f:\n",
        "        f.write(uploaded_file.getbuffer())\n",
        "\n",
        "    st.success(f\"–§–∞–π–ª '{uploaded_file.name}' —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω!\")\n",
        "\n",
        "    if st.button(\"–ù–∞—á–∞—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É\"):\n",
        "        st.info(\"–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–∞—á–∞–ª–∞—Å—å...\")\n",
        "\n",
        "        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "        df = load_data(file_path)\n",
        "\n",
        "        if df is not None:\n",
        "            # –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–æ–ª–±—Ü–æ–≤\n",
        "            df, column_mapping = validate_data(df)\n",
        "\n",
        "            if df is not None and column_mapping:\n",
        "                # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ –∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤\n",
        "                unique_df, duplicates_detailed, work_df = find_unique_and_duplicates(df, column_mapping)\n",
        "\n",
        "                # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞\n",
        "                output_path = Path('–æ—Ç—á–µ—Ç_–æ–±—Ä–∞—â–µ–Ω–∏–π.xlsx')\n",
        "                success = generate_report(df, unique_df, duplicates_detailed, work_df, column_mapping)\n",
        "\n",
        "                if success:\n",
        "                    st.success(\"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –û—Ç—á–µ—Ç –≥–æ—Ç–æ–≤.\")\n",
        "\n",
        "                    # –ö–Ω–æ–ø–∫–∞ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –æ—Ç—á–µ—Ç–∞\n",
        "                    with open(output_path, \"rb\") as f:\n",
        "                        st.download_button(\n",
        "                            label=\"–°–∫–∞—á–∞—Ç—å –æ—Ç—á–µ—Ç\",\n",
        "                            data=f,\n",
        "                            file_name=\"–æ—Ç—á–µ—Ç_–æ–±—Ä–∞—â–µ–Ω–∏–π.xlsx\",\n",
        "                            mime=\"application/vnd.openxmlformats-officedocument.spreadsheet.sheet\"\n",
        "                        )\n",
        "\n",
        "                    # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Å—é–¥–∞ –≤—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏)\n",
        "                    st.subheader(\"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏\")\n",
        "                    st.write(f\"–í—Å–µ–≥–æ –æ–±—Ä–∞—â–µ–Ω–∏–π: {len(df)}\")\n",
        "                    st.write(f\"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –æ–±—Ä–∞—â–µ–Ω–∏–π: {len(unique_df)}\")\n",
        "                    st.write(f\"–î—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(duplicates_detailed)}\")\n",
        "\n",
        "                else:\n",
        "                    st.error(\"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–∏ –æ—Ç—á–µ—Ç–∞.\")\n",
        "            else:\n",
        "                st.error(\"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Å—Ç–æ–ª–±—Ü—ã –∏–ª–∏ –¥–∞–Ω–Ω—ã–µ –Ω–µ –ø—Ä–æ—à–ª–∏ –≤–∞–ª–∏–¥–∞—Ü–∏—é.\")\n",
        "        else:\n",
        "            st.error(\"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ñ–∞–π–ª–∞.\")\n",
        "\n",
        "        # –û—á–∏—Å—Ç–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
        "        # os.remove(file_path)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-09-25 12:06:55.291 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 12:06:55.293 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 12:06:55.295 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 12:06:55.296 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 12:06:55.298 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 12:06:55.300 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 12:06:55.301 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 12:06:55.303 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 12:06:55.304 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 12:06:55.306 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 12:06:55.307 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 12:06:55.308 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34cacc3b",
        "outputId": "59616e8b-9f5a-4740-dc26-fe6bcc4c4c65"
      },
      "source": [
        "# To run the streamlit app, execute this cell.\n",
        "!python -m streamlit run /content/drive/MyDrive/Colab Notebooks/your_notebook_name.ipynb --share\n",
        "# Replace 'your_notebook_name.ipynb' with the actual name of your notebook file.\n",
        "# If you are running this locally, you can simply use:\n",
        "# !python -m streamlit run your_notebook_name.ipynb"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: streamlit run [OPTIONS] TARGET [ARGS]...\n",
            "Try 'streamlit run --help' for help.\n",
            "\n",
            "Error: No such option: --share Did you mean --help?\n"
          ]
        }
      ]
    }
  ]
}